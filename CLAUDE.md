

# CaseHOLD Distillation Research

This repository implements a comprehensive **Knowledge Distillation** study to compress the `Legal-BERT` model while retaining performance on the **CaseHOLD** legal reasoning benchmark.

The project follows a rigorous two-stage experimental design:
- **Stage 1:** Vanilla Knowledge Distillation (grid search over Î± and temperature)
- **Stage 2:** Patient Knowledge Distillation with skip strategy (grid search over student sizes and Î²)

All experiments utilize a strict **80/10/10 data split** to prevent test set leakage ("The Vault").

## ðŸ“‚ Project Structure

```text
.
â”œâ”€â”€ data/                          # Data storage (Gitignored)
â”‚   â””â”€â”€ casehold/                  # Created by create_splits.py
â”‚       â”œâ”€â”€ train.csv              # 80% Training set
â”‚       â”œâ”€â”€ dev.csv                # 10% Validation set (for hyperparameter tuning)
â”‚       â””â”€â”€ test.csv               # 10% Test set (THE VAULT - Only for final evaluation)
â”‚
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ training_runs/             # All trained models
â”‚   â”‚   â”œâ”€â”€ fine_tuned_base_bert_legal_teacher/  # Teacher model (12-layer)
â”‚   â”‚   â”œâ”€â”€ vanilla_kd_grid_search/              # Stage 1: 9 vanilla KD runs
â”‚   â”‚   â””â”€â”€ pkd_skip_grid_search/                # Stage 2: 16 PKD runs
â”‚   â”œâ”€â”€ eval_results/              # Evaluation CSVs and summaries
â”‚   â”‚   â”œâ”€â”€ vanilla_kd_grid_search_eval.csv
â”‚   â”‚   â”œâ”€â”€ pkd_skip_grid_search_eval.csv
â”‚   â”‚   â”œâ”€â”€ test_set_evaluation.csv
â”‚   â”‚   â””â”€â”€ *.txt (summary reports)
â”‚   â””â”€â”€ plots/                     # Visualizations (heatmaps, learning curves)
â”‚
â”œâ”€â”€ create_splits.py               # Deterministic data splitting script
â”œâ”€â”€ data_loader.py                 # HF Datasets pipeline for Multiple Choice
â”œâ”€â”€ model_utils.py                 # Student model initialization (layer truncation)
â”œâ”€â”€ pkd_loss.py                    # Patient Distillation Loss (Cheng et al. 2019)
â”‚
â”œâ”€â”€ train_teacher_cloud.py         # Teacher training script
â”œâ”€â”€ vanilla_distill_cloud.py       # Stage 1: Vanilla KD grid search
â”œâ”€â”€ distill_cloud.py               # Stage 2: PKD-skip grid search
â”‚
â”œâ”€â”€ evaluate_grid_search.py        # Vanilla KD post-hoc evaluation
â”œâ”€â”€ evaluate_pkd_grid_search.py    # PKD post-hoc evaluation
â”œâ”€â”€ analyze_grid_search.py         # Vanilla KD analysis & visualization
â”œâ”€â”€ analyze_pkd_grid_search.py     # PKD analysis & visualization
â”œâ”€â”€ evaluate_on_test.py            # Final test set evaluation (THE VAULT)
â””â”€â”€ Interpretability_Dev_EvalxNLP.ipynb  # Attribution analysis (teacher vs student)
```

## ðŸš€ Setup

1.  **Install Dependencies**

    ```bash
    pip install torch transformers datasets accelerate scikit-learn numpy
    ```

2.  **Prepare the Data**
    Download `casehold.csv` and place it in the root (or update the script path). Then run the splitter to generate the canonical datasets.

    ```bash
    python create_splits.py
    ```

    *Output:* Creates `data/casehold/train.csv`, `dev.csv`, and `test.csv`.

## ðŸ§ª Research Workflow

### Stage 0: Train the Teacher Model

Fine-tune the full 12-layer `nlpaueb/legal-bert-base-uncased` model on CaseHOLD.

**Script:** `train_teacher_cloud.py`

```bash
python train_teacher_cloud.py
```

**Configuration:**
- Learning rate: 1e-5 (optimal from LR search)
- Batch size: 32
- Epochs: 4
- Evaluation: On dev set only
- **Result:** 75.48% dev accuracy

**Output:** `results/training_runs/fine_tuned_base_bert_legal_teacher/run_lr_1e-05/checkpoint-1325`

---

### Stage 1: Vanilla Knowledge Distillation Grid Search

Find optimal Î± (distillation weight) and temperature for 6-layer student.

**Script:** `vanilla_distill_cloud.py`

```bash
python vanilla_distill_cloud.py
```

**Grid Search:**
- Alphas: [0.2, 0.5, 0.7]
- Temperatures: [5, 10, 20]
- **Total:** 9 runs Ã— 4 epochs = 36 checkpoints

**Loss Function:**
```
L_total = (1-Î±)Â·L_CE + Î±Â·L_KD
where L_KD = KL_div(student_logits/T || teacher_logits/T) Ã— TÂ²
```

**Post-Training Evaluation:**
```bash
# Evaluate all checkpoints on dev set
python evaluate_grid_search.py

# Analyze results and create visualizations
python analyze_grid_search.py
```

**Best Result:** Î±=0.7, T=20, Epoch 3 â†’ **75.63% dev accuracy**

---

### Stage 2: Patient Knowledge Distillation Grid Search

Find optimal Î² (patient loss weight) for each student size, using best Î± and T from Stage 1.

**Script:** `distill_cloud.py`

```bash
python distill_cloud.py
```

**Grid Search:**
- Student layers: [6, 4, 3, 2]
- Betas: [10, 100, 500, 1000]
- **Total:** 16 runs Ã— 4 epochs = 64 checkpoints
- **Fixed:** Î±=0.7, T=20 (from Stage 1)

**Loss Function:**
```
L_total = (1-Î±)Â·L_CE + Î±Â·L_KD + Î²Â·L_PT
where L_PT = Î£ ||normalize(h_student) - normalize(h_teacher)||Â²
```

**PKD Strategy:** Skip - student layers match evenly-spaced teacher layers
- 6-layer: matches teacher layers 0, 2, 4, 6, 8, 10
- 4-layer: matches teacher layers 0, 3, 6, 9
- 3-layer: matches teacher layers 0, 4, 8
- 2-layer: matches teacher layers 0, 6

**Post-Training Evaluation:**
```bash
# Evaluate all PKD checkpoints on dev set
python evaluate_pkd_grid_search.py

# Analyze results and create visualizations
python analyze_pkd_grid_search.py
```

---

### Stage 3: Final Test Set Evaluation

Evaluate best models on the held-out test set (THE VAULT).

**Script:** `evaluate_on_test.py`

```bash
# Recommended: Only evaluate best models
python evaluate_on_test.py --best-only

# Or evaluate a specific model
python evaluate_on_test.py --model-path "path/to/checkpoint"
```

**Models Evaluated:**
- Teacher (12-layer baseline)
- Best vanilla KD (6-layer)
- Best PKD per student size (6, 4, 3, 2 layers)

**IMPORTANT:** Run this **only once** for final results. Test set should never be used for model selection or hyperparameter tuning.

## ðŸ› ï¸ Technical Implementation Details

### Data Loading (`data_loader.py`)

  * **Task:** Multiple Choice (5 options per prompt).
  * **Formatting:** Expands each example into 5 separate input sequences `[CLS] Context [SEP] Option [SEP]`.
  * **Labels:** Converts the dataset's string labels ("0"-"4") into integer indices for CrossEntropyLoss.

### Student Initialization (`model_utils.py`)

We do not use a generic "small BERT." We explicitly construct the student by slicing the Teacher:

```python
# Copies layers 0, 1, 2, 3, 4, 5 from Teacher -> Student
student.bert.encoder.layer[i].load_state_dict(teacher.bert.encoder.layer[i].state_dict())
```

### Patient Loss (`pkd_loss.py`)

Implements the normalized Mean Squared Error from *Cheng et al. (2019)*:
$$L_{PT} = \sum || \frac{h_s}{||h_s||_2} - \frac{h_t}{||h_t||_2} ||_2^2$$

**Strategies:**
- `skip`: Student layers match evenly-spaced teacher layers (used in this project)
- `last`: Student layers match final k teacher layers

### Custom Trainers

**`PKDTrainer` (in `distill_cloud.py`):**
Combines three loss components:
```python
total_loss = (1-Î±)Â·task_loss + Î±Â·distill_loss + Î²Â·pkd_loss
```
- Task loss: Cross-entropy with hard labels
- Distillation loss: KL divergence with soft teacher predictions
- Patient loss: Intermediate layer matching

**Memory Optimization:**
- `save_only_model=True`: Only saves model weights, not optimizer/scheduler states
- Reduces checkpoint size by ~70% (critical for grid searches)
- Enables 16 PKD runs Ã— 4 checkpoints to fit in 50GB storage

## ðŸ“Š Evaluation Strategy

### Dev Set Evaluation (Model Selection)

All hyperparameter tuning and model selection uses **only the dev set**.

**Post-hoc evaluation approach:**
- Training runs use `eval_strategy="no"` to avoid memory issues
- After training completes, evaluate all checkpoints separately
- Enables memory-efficient evaluation with aggressive cleanup

**Scripts:**
- `evaluate_grid_search.py` - Vanilla KD checkpoints
- `evaluate_pkd_grid_search.py` - PKD checkpoints

**Configuration:**
- Batch size: 4 (conservative for memory constraints)
- FP16 precision
- Aggressive memory cleanup between evaluations
- CSV crash recovery (can resume if interrupted)

### Test Set Evaluation (Final Results)

The test set ("THE VAULT") is **only** used for final evaluation.

**Rules:**
1. Never loaded during training
2. Never used for hyperparameter tuning
3. Only evaluated **once** after all model selection is complete
4. Results reported in final paper/analysis

**Script:** `evaluate_on_test.py --best-only`

## ðŸ“ˆ Key Results

### Teacher (Baseline)
- 12-layer Legal-BERT
- **75.48% dev accuracy**

### Stage 1: Vanilla KD (6-layer)
- Best config: Î±=0.7, T=20, Epoch 3
- **75.63% dev accuracy**
- Matches teacher performance with 50% model compression

### Stage 2: Patient KD
- Results vary by student size and Î²
- See `results/eval_results/pkd_best_models_summary.txt` for details
- Comparison with vanilla KD in `pkd_vs_vanilla_comparison.txt`

## ðŸŽ¯ Best Practices Learned

### Training at Scale
1. **Disable evaluation during training** when memory-constrained
2. **Save checkpoints frequently** (every epoch) for post-hoc analysis
3. **Use `save_only_model=True`** to reduce storage by 70%
4. **Grid search systematically** - Stage 1 (Î±, T) â†’ Stage 2 (Î² per size)

### Evaluation
1. **Two-tiered strategy** reduces evaluations by 75%:
   - Tier 1: Final checkpoints only
   - Tier 2: All epochs for top performers
2. **Aggressive memory cleanup** between evaluations
3. **CSV checkpointing** for crash recovery
4. **Separate dev/test strictly** - never touch test set until final

### Student Model Architecture
- **Factor of 12 preferred** for clean layer matching (6, 4, 3, 2)
- **Layer truncation initialization** from fine-tuned teacher
- **Skip strategy** works well for varying student sizes