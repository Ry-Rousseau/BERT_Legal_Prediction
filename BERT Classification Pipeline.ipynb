{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ba024a9",
   "metadata": {},
   "source": [
    "### Clean, Format and Convert all facts texts into BERT embeddings\n",
    "\n",
    "Alter BERT models at this **feature engineering** phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3b3ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION PARAMETERS\n",
    "# ============================================================================\n",
    "\n",
    "# Data Configuration\n",
    "DATA_PATH = \"C:\\\\Users\\\\rhrou\\\\Downloads\\\\justice.csv\"\n",
    "TARGET_COLUMN = 'first_party_winner'\n",
    "TEXT_COLUMN = 'facts'\n",
    "\n",
    "# Train-Test Split Configuration\n",
    "TEST_SIZE = 0.25\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# BERT Configuration\n",
    "# Options: 'bert-base-uncased', 'nlpaueb/legal-bert-base-uncased'\n",
    "BERT_MODEL_NAME = 'bert-base-uncased'  \n",
    "\n",
    "BERT_MAX_LENGTH = 512\n",
    "BERT_BATCH_SIZE = 40\n",
    "\n",
    "# Text Preprocessing Configuration\n",
    "LOWERCASE = True  # Set to True for 'uncased' models, False for 'cased' models\n",
    "NORMALIZE_UNICODE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd3ac452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "BERT FEATURE PREPARATION PIPELINE\n",
      "======================================================================\n",
      "\n",
      "[1/6] Loading data from: C:\\Users\\rhrou\\Downloads\\justice.csv\n",
      "      Loaded: 3303 cases, 16 columns\n",
      "\n",
      "[2/6] Removing missing values...\n",
      "      Retained: 3098 cases (93.8%)\n",
      "\n",
      "[3/6] Preprocessing text data...\n",
      "      Final dataset shape: (3098, 2)\n",
      "      Target distribution: {1: 0.6672046481601033, 0: 0.3327953518398967}\n",
      "\n",
      "[4/6] Splitting data (test_size=0.25)...\n",
      "      Training set: 2323 cases\n",
      "      Test set: 775 cases\n",
      "\n",
      "[5/6] Initializing BERT model: bert-base-uncased\n",
      "[info] torch.compile disabled: name 'platform' is not defined\n",
      "      Device: cpu\n",
      "\n",
      "[6/6] Extracting BERT features...\n",
      "      Processing training set...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 207\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;66;03m# ============================================================================\u001b[39;00m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;66;03m# ENTRY POINT\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;66;03m# ============================================================================\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 207\u001b[0m     X_train_bert, X_test_bert, y_train, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;66;03m# Save features for next stage (optional)\u001b[39;00m\n\u001b[0;32m    210\u001b[0m     \u001b[38;5;66;03m# np.save('X_train_bert.npy', X_train_bert)\u001b[39;00m\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;66;03m# np.save('X_test_bert.npy', X_test_bert)\u001b[39;00m\n\u001b[0;32m    212\u001b[0m     \u001b[38;5;66;03m# np.save('y_train.npy', y_train)\u001b[39;00m\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;66;03m# np.save('y_test.npy', y_test)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[7], line 166\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m[6/6] Extracting BERT features...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m      Processing training set...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 166\u001b[0m X_train_bert \u001b[38;5;241m=\u001b[39m \u001b[43mbert_extractor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBERT_BATCH_SIZE\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m      Processing test set...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    172\u001b[0m X_test_bert \u001b[38;5;241m=\u001b[39m bert_extractor\u001b[38;5;241m.\u001b[39mextract_features(\n\u001b[0;32m    173\u001b[0m     X_test\u001b[38;5;241m.\u001b[39mtolist(),\n\u001b[0;32m    174\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mBERT_BATCH_SIZE\n\u001b[0;32m    175\u001b[0m )\n",
      "Cell \u001b[1;32mIn[7], line 88\u001b[0m, in \u001b[0;36mBERTFeatureExtractor.extract_features\u001b[1;34m(self, texts, batch_size, pool)\u001b[0m\n\u001b[0;32m     86\u001b[0m         out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(input_ids\u001b[38;5;241m=\u001b[39mids, attention_mask\u001b[38;5;241m=\u001b[39mmask)\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 88\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     90\u001b[0m last \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pool \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcls\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\rhrou\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rhrou\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\rhrou\\miniconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1000\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[0;32m    993\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[0;32m    994\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[0;32m    995\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[0;32m    996\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[0;32m    997\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[0;32m    998\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m-> 1000\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1001\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1002\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1003\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1004\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1005\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1006\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1007\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1008\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1009\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1010\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1011\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1012\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1013\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1014\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rhrou\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rhrou\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\rhrou\\miniconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:650\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[0;32m    646\u001b[0m     all_hidden_states \u001b[38;5;241m=\u001b[39m all_hidden_states \u001b[38;5;241m+\u001b[39m (hidden_states,)\n\u001b[0;32m    648\u001b[0m layer_head_mask \u001b[38;5;241m=\u001b[39m head_mask[i] \u001b[38;5;28;01mif\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 650\u001b[0m layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    651\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    652\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    653\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    654\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# as a positional argument for gradient checkpointing\u001b[39;49;00m\n\u001b[0;32m    655\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    656\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    657\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    658\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    659\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    661\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    662\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[1;32mc:\\Users\\rhrou\\miniconda3\\Lib\\site-packages\\transformers\\modeling_layers.py:94\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     91\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning_once(message)\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m---> 94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rhrou\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rhrou\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\rhrou\\miniconda3\\Lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[0;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rhrou\\miniconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:588\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, output_attentions, cache_position)\u001b[0m\n\u001b[0;32m    585\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    586\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m outputs \u001b[38;5;241m+\u001b[39m cross_attention_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add cross attentions if we output attention weights\u001b[39;00m\n\u001b[1;32m--> 588\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    589\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[0;32m    590\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    591\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[0;32m    593\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32mc:\\Users\\rhrou\\miniconda3\\Lib\\site-packages\\transformers\\pytorch_utils.py:257\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[1;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[0;32m    254\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[0;32m    255\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[1;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rhrou\\miniconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:597\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[1;34m(self, attention_output)\u001b[0m\n\u001b[0;32m    595\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[0;32m    596\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate(attention_output)\n\u001b[1;32m--> 597\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mintermediate_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    598\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[1;32mc:\\Users\\rhrou\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rhrou\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\rhrou\\miniconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:525\u001b[0m, in \u001b[0;36mBertOutput.forward\u001b[1;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[0;32m    524\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor, input_tensor: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m--> 525\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    526\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[0;32m    527\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(hidden_states \u001b[38;5;241m+\u001b[39m input_tensor)\n",
      "File \u001b[1;32mc:\\Users\\rhrou\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rhrou\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\rhrou\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m    131\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;124;03m    Runs the forward pass.\u001b[39;00m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "BERT Feature Preparation for Legal Outcome Prediction\n",
    "Streamlined script for data loading, preprocessing, and BERT feature extraction\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ============================================================================\n",
    "# PREPROCESSING FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "def preprocess_text(text, lowercase=True, normalize=True):\n",
    "    \"\"\"\n",
    "    Minimal cleaning for BERT:\n",
    "    - Trim whitespace\n",
    "    - Optional Unicode normalization (NFKC)\n",
    "    - Optional lowercasing (use True for uncased models; False for cased models)\n",
    "    \"\"\"\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "    text = str(text).strip()\n",
    "    if normalize:\n",
    "        text = unicodedata.normalize(\"NFKC\", text)\n",
    "    if lowercase:\n",
    "        text = text.lower()\n",
    "    return text\n",
    "\n",
    "# ============================================================================\n",
    "# BERT FEATURE EXTRACTOR CLASS\n",
    "# ============================================================================\n",
    "\n",
    "class BERTFeatureExtractor:\n",
    "    def __init__(self, model_name='bert-base-uncased', max_length=512, precision='fp16'):\n",
    "        self.model_name = model_name\n",
    "        self.max_length = max_length\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.precision = precision.lower()\n",
    "\n",
    "        if self.precision == 'fp16':\n",
    "            self.torch_dtype = torch.float16\n",
    "        elif self.precision == 'bf16':\n",
    "            self.torch_dtype = torch.bfloat16\n",
    "        else:\n",
    "            self.torch_dtype = torch.float32\n",
    "\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "        self.model = BertModel.from_pretrained(model_name, torch_dtype=self.torch_dtype, low_cpu_mem_usage=True)\n",
    "        self.model.to(self.device).eval()\n",
    "\n",
    "        # Optional: BetterTransformer is fine on Windows\n",
    "        try: self.model = self.model.to_bettertransformer()\n",
    "        except Exception: pass\n",
    "\n",
    "        # Only try compile on non-Windows (avoid 'cl not found')\n",
    "        try:\n",
    "            if platform.system() != \"Windows\":\n",
    "                self.model = torch.compile(self.model)\n",
    "        except Exception as e:\n",
    "            print(f\"[info] torch.compile disabled: {e}\")\n",
    "\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "    def extract_features(self, texts, batch_size=32, pool='cls'):\n",
    "        use_autocast = (self.device.type == 'cuda' and self.torch_dtype in (torch.float16, torch.bfloat16))\n",
    "        all_embeddings = []\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i+batch_size]\n",
    "            enc = self.tokenizer(batch, padding='max_length', truncation=True,\n",
    "                                 max_length=self.max_length, return_tensors='pt')\n",
    "            ids = enc['input_ids'].to(self.device, non_blocking=True)\n",
    "            mask = enc['attention_mask'].to(self.device, non_blocking=True)\n",
    "\n",
    "            with torch.inference_mode():\n",
    "                if use_autocast:\n",
    "                    with torch.autocast(device_type='cuda', dtype=self.torch_dtype):\n",
    "                        out = self.model(input_ids=ids, attention_mask=mask)\n",
    "                else:\n",
    "                    out = self.model(input_ids=ids, attention_mask=mask)\n",
    "\n",
    "                last = out.last_hidden_state\n",
    "                if pool == 'cls':\n",
    "                    feats = last[:, 0, :]\n",
    "                else:\n",
    "                    m = mask.unsqueeze(-1)\n",
    "                    feats = (last * m).sum(1) / m.sum(1).clamp_min(1)\n",
    "\n",
    "            all_embeddings.append(feats.detach().cpu().numpy())\n",
    "        return np.vstack(all_embeddings)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    print(\"=\"*70)\n",
    "    print(\"BERT FEATURE PREPARATION PIPELINE\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # 1. Load Data\n",
    "    print(f\"\\n[1/6] Loading data from: {DATA_PATH}\")\n",
    "    df = pd.read_csv(DATA_PATH)\n",
    "    print(f\"      Loaded: {df.shape[0]} cases, {df.shape[1]} columns\")\n",
    "    \n",
    "    # 2. Remove Missing Values\n",
    "    print(f\"\\n[2/6] Removing missing values...\")\n",
    "    initial_count = df.shape[0]\n",
    "    df.dropna(inplace=True)\n",
    "    print(f\"      Retained: {df.shape[0]} cases ({df.shape[0]/initial_count*100:.1f}%)\")\n",
    "    \n",
    "    # 4. Extract and Clean Text\n",
    "    print(f\"\\n[3/6] Preprocessing text data...\")\n",
    "    df_nlp = df[[TEXT_COLUMN]].copy()\n",
    "    \n",
    "    # Remove HTML tags\n",
    "    df_nlp[TEXT_COLUMN] = df_nlp[TEXT_COLUMN].str.replace(r'<[^<>]*>', '', regex=True)\n",
    "    \n",
    "    # Apply text preprocessing\n",
    "    df_nlp['facts_clean'] = df_nlp[TEXT_COLUMN].apply(\n",
    "        lambda x: preprocess_text(x, lowercase=LOWERCASE, normalize=NORMALIZE_UNICODE)\n",
    "    )\n",
    "    \n",
    "    # 6. Combine with Target Variable\n",
    "    df_target = df[[TARGET_COLUMN]].copy()\n",
    "    df_target[TARGET_COLUMN] = df_target[TARGET_COLUMN].astype(int)\n",
    "    df_final = pd.concat([df_nlp[['facts_clean']], df_target], axis=1, join='inner')\n",
    "    \n",
    "    print(f\"      Final dataset shape: {df_final.shape}\")\n",
    "    print(f\"      Target distribution: {df_final[TARGET_COLUMN].value_counts(normalize=True).to_dict()}\")\n",
    "    \n",
    "    # 7. Prepare Features and Target\n",
    "    X = df_final['facts_clean']\n",
    "    y = df_final[TARGET_COLUMN]\n",
    "    \n",
    "    # 10. Train-Test Split\n",
    "    print(f\"\\n[4/6] Splitting data (test_size={TEST_SIZE})...\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y,\n",
    "        test_size=TEST_SIZE,\n",
    "        random_state=RANDOM_STATE,\n",
    "        stratify=y\n",
    "    )\n",
    "    print(f\"      Training set: {len(X_train)} cases\")\n",
    "    print(f\"      Test set: {len(X_test)} cases\")\n",
    "    \n",
    "    # 11. Extract BERT Features\n",
    "    print(f\"\\n[5/6] Initializing BERT model: {BERT_MODEL_NAME}\")\n",
    "    bert_extractor = BERTFeatureExtractor(\n",
    "        model_name=BERT_MODEL_NAME,\n",
    "        max_length=BERT_MAX_LENGTH\n",
    "    )\n",
    "    print(f\"      Device: {bert_extractor.device}\")\n",
    "    \n",
    "    print(f\"\\n[6/6] Extracting BERT features...\")\n",
    "    print(\"      Processing training set...\")\n",
    "    X_train_bert = bert_extractor.extract_features(\n",
    "        X_train.tolist(),\n",
    "        batch_size=BERT_BATCH_SIZE\n",
    "    )\n",
    "    \n",
    "    print(\"      Processing test set...\")\n",
    "    X_test_bert = bert_extractor.extract_features(\n",
    "        X_test.tolist(),\n",
    "        batch_size=BERT_BATCH_SIZE\n",
    "    )\n",
    "    \n",
    "    # Final Summary\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"FEATURE EXTRACTION COMPLETE\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nConfiguration:\")\n",
    "    print(f\"  - BERT Model: {BERT_MODEL_NAME}\")\n",
    "    print(f\"  - Max Length: {BERT_MAX_LENGTH}\")\n",
    "    print(f\"  - Batch Size: {BERT_BATCH_SIZE}\")\n",
    "    print(f\"  - Test Split: {TEST_SIZE*100:.0f}%\")\n",
    "    \n",
    "    print(f\"\\nOutput Shapes:\")\n",
    "    print(f\"  - X_train_bert: {X_train_bert.shape} (samples Ã— features)\")\n",
    "    print(f\"  - X_test_bert:  {X_test_bert.shape}\")\n",
    "    print(f\"  - y_train:      {y_train.shape}\")\n",
    "    print(f\"  - y_test:       {y_test.shape}\")\n",
    "    \n",
    "    print(f\"\\nTarget Distribution:\")\n",
    "    print(f\"  - Training:   {y_train.value_counts(normalize=True).to_dict()}\")\n",
    "    print(f\"  - Test:       {y_test.value_counts(normalize=True).to_dict()}\")\n",
    "    \n",
    "    print(f\"\\nFeature vector dimension: {X_train_bert.shape[1]}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    return X_train_bert, X_test_bert, y_train, y_test\n",
    "\n",
    "# ============================================================================\n",
    "# ENTRY POINT\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    X_train_bert, X_test_bert, y_train, y_test = main()\n",
    "    \n",
    "    # Save features for next stage (optional)\n",
    "    # np.save('X_train_bert.npy', X_train_bert)\n",
    "    # np.save('X_test_bert.npy', X_test_bert)\n",
    "    # np.save('y_train.npy', y_train)\n",
    "    # np.save('y_test.npy', y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "106689a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda_available: False\n",
      "torch.version.cuda: None\n",
      "device_count: 0\n"
     ]
    }
   ],
   "source": [
    "import torch, platform\n",
    "print(\"cuda_available:\", torch.cuda.is_available())\n",
    "print(\"torch.version.cuda:\", torch.version.cuda)\n",
    "print(\"device_count:\", torch.cuda.device_count())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"gpu:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eeecf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Legal Outcome Classification with BERT Features\n",
    "Streamlined script for training and evaluating classification models on BERT embeddings\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION PARAMETERS\n",
    "# ============================================================================\n",
    "\n",
    "# Input Data Configuration\n",
    "# Option 1: Load from saved arrays\n",
    "LOAD_FROM_FILES = False\n",
    "X_TRAIN_PATH = 'X_train_bert.npy'\n",
    "X_TEST_PATH = 'X_test_bert.npy'\n",
    "Y_TRAIN_PATH = 'y_train.npy'\n",
    "Y_TEST_PATH = 'y_test.npy'\n",
    "\n",
    "# Option 2: Assume variables are already in memory from previous script\n",
    "# Set LOAD_FROM_FILES = False and ensure X_train_bert, X_test_bert, y_train, y_test exist\n",
    "\n",
    "# Classifier Configuration\n",
    "CLASSIFIERS_TO_TRAIN = ['logistic', 'random_forest', 'knn']  # Options: 'logistic', 'random_forest', 'knn'\n",
    "\n",
    "# Logistic Regression Parameters\n",
    "LR_C = 1.0  # Regularization strength (lower = stronger regularization)\n",
    "LR_MAX_ITER = 1000\n",
    "LR_SOLVER = 'liblinear'  # Options: 'liblinear', 'lbfgs', 'saga'\n",
    "\n",
    "# Random Forest Parameters\n",
    "RF_N_ESTIMATORS = 100  # Number of trees\n",
    "RF_MAX_DEPTH = None  # Maximum tree depth (None = no limit)\n",
    "RF_MIN_SAMPLES_SPLIT = 2\n",
    "RF_MIN_SAMPLES_LEAF = 1\n",
    "\n",
    "# K-Nearest Neighbors Parameters\n",
    "KNN_N_NEIGHBORS = 5  # Number of neighbors\n",
    "KNN_WEIGHTS = 'uniform'  # Options: 'uniform', 'distance'\n",
    "KNN_METRIC = 'minkowski'  # Options: 'euclidean', 'manhattan', 'minkowski'\n",
    "\n",
    "# Baseline Comparison Configuration\n",
    "RUN_BASELINE = True  # Compare with bag-of-words baseline\n",
    "BASELINE_MAX_FEATURES = 5000\n",
    "\n",
    "# Output Configuration\n",
    "SAVE_RESULTS = True\n",
    "RESULTS_PATH = 'classification_results.csv'\n",
    "PLOT_RESULTS = True\n",
    "SAVE_PLOTS = True\n",
    "PLOT_PATH = 'model_comparison.png'\n",
    "\n",
    "# Random State for Reproducibility\n",
    "RANDOM_STATE = 42\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a94e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# CLASSIFIER DEFINITIONS\n",
    "# ============================================================================\n",
    "\n",
    "def get_classifiers():\n",
    "    \"\"\"Initialize classifiers with configured parameters\"\"\"\n",
    "    classifiers = {}\n",
    "    \n",
    "    if 'logistic' in CLASSIFIERS_TO_TRAIN:\n",
    "        classifiers['Logistic Regression'] = LogisticRegression(\n",
    "            C=LR_C,\n",
    "            solver=LR_SOLVER,\n",
    "            max_iter=LR_MAX_ITER,\n",
    "            random_state=RANDOM_STATE\n",
    "        )\n",
    "    \n",
    "    if 'random_forest' in CLASSIFIERS_TO_TRAIN:\n",
    "        classifiers['Random Forest'] = RandomForestClassifier(\n",
    "            n_estimators=RF_N_ESTIMATORS,\n",
    "            max_depth=RF_MAX_DEPTH,\n",
    "            min_samples_split=RF_MIN_SAMPLES_SPLIT,\n",
    "            min_samples_leaf=RF_MIN_SAMPLES_LEAF,\n",
    "            random_state=RANDOM_STATE,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "    \n",
    "    if 'knn' in CLASSIFIERS_TO_TRAIN:\n",
    "        classifiers['K-Nearest Neighbors'] = KNeighborsClassifier(\n",
    "            n_neighbors=KNN_N_NEIGHBORS,\n",
    "            weights=KNN_WEIGHTS,\n",
    "            metric=KNN_METRIC,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "    \n",
    "    return classifiers\n",
    "\n",
    "# ============================================================================\n",
    "# EVALUATION FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def evaluate_classifier(name, clf, X_train, X_test, y_train, y_test):\n",
    "    \"\"\"Train and evaluate a single classifier\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Training: {name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Train\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\nAccuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "    print(f\"\\nClassification Report:\")\n",
    "    print(classification_report(\n",
    "        y_test, y_pred,\n",
    "        target_names=['First Party Lost (0)', 'First Party Won (1)'],\n",
    "        digits=4\n",
    "    ))\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(f\"\\nConfusion Matrix:\")\n",
    "    print(cm)\n",
    "    \n",
    "    return accuracy, y_pred, cm\n",
    "\n",
    "def plot_results(results, save_path=None):\n",
    "    \"\"\"Visualize model performance comparison\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    models = list(results.keys())\n",
    "    accuracies = [results[m] for m in models]\n",
    "    \n",
    "    colors = ['steelblue', 'coral', 'mediumseagreen', 'orchid', 'gold']\n",
    "    bars = ax.barh(models, accuracies, color=colors[:len(models)])\n",
    "    \n",
    "    ax.set_xlabel('Accuracy', fontsize=12, fontweight='bold')\n",
    "    ax.set_title('Model Performance Comparison (BERT Features)', \n",
    "                 fontsize=14, fontweight='bold', pad=20)\n",
    "    ax.set_xlim([0, 1.0])\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, (bar, acc) in enumerate(zip(bars, accuracies)):\n",
    "        ax.text(acc + 0.01, i, f'{acc:.4f}', \n",
    "                va='center', fontsize=11, fontweight='bold')\n",
    "    \n",
    "    plt.grid(axis='x', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"\\nPlot saved to: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def run_baseline_comparison(X_train_text, X_test_text, y_train, y_test):\n",
    "    \"\"\"Run bag-of-words baseline for comparison\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"BASELINE: CountVectorizer (Bag-of-Words)\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    vectorizer = CountVectorizer(max_features=BASELINE_MAX_FEATURES)\n",
    "    X_train_bow = vectorizer.fit_transform(X_train_text)\n",
    "    X_test_bow = vectorizer.transform(X_test_text)\n",
    "    \n",
    "    print(f\"\\nBag-of-Words feature shape: {X_train_bow.shape}\")\n",
    "    \n",
    "    baseline_clf = LogisticRegression(\n",
    "        solver=LR_SOLVER,\n",
    "        max_iter=LR_MAX_ITER,\n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "    baseline_clf.fit(X_train_bow, y_train)\n",
    "    y_pred_baseline = baseline_clf.predict(X_test_bow)\n",
    "    baseline_accuracy = accuracy_score(y_test, y_pred_baseline)\n",
    "    \n",
    "    print(f\"\\nBaseline Accuracy: {baseline_accuracy:.4f} ({baseline_accuracy*100:.2f}%)\")\n",
    "    \n",
    "    return baseline_accuracy\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "def main(X_train_bert=None, X_test_bert=None, y_train=None, y_test=None, \n",
    "         X_train_text=None, X_test_text=None):\n",
    "    \"\"\"\n",
    "    Main classification pipeline\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train_bert : array, shape (n_train_samples, n_features)\n",
    "        Training BERT features\n",
    "    X_test_bert : array, shape (n_test_samples, n_features)\n",
    "        Test BERT features\n",
    "    y_train : array, shape (n_train_samples,)\n",
    "        Training labels\n",
    "    y_test : array, shape (n_test_samples,)\n",
    "        Test labels\n",
    "    X_train_text : array-like (optional)\n",
    "        Original training text for baseline comparison\n",
    "    X_test_text : array-like (optional)\n",
    "        Original test text for baseline comparison\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"CLASSIFICATION MODELING PIPELINE\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Load data if needed\n",
    "    if LOAD_FROM_FILES:\n",
    "        print(f\"\\n[1/4] Loading BERT features from files...\")\n",
    "        X_train_bert = np.load(X_TRAIN_PATH)\n",
    "        X_test_bert = np.load(X_TEST_PATH)\n",
    "        y_train = np.load(Y_TRAIN_PATH)\n",
    "        y_test = np.load(Y_TEST_PATH)\n",
    "        print(f\"      Loaded successfully\")\n",
    "    else:\n",
    "        print(f\"\\n[1/4] Using BERT features from memory...\")\n",
    "    \n",
    "    print(f\"      X_train_bert: {X_train_bert.shape}\")\n",
    "    print(f\"      X_test_bert:  {X_test_bert.shape}\")\n",
    "    print(f\"      y_train:      {y_train.shape}\")\n",
    "    print(f\"      y_test:       {y_test.shape}\")\n",
    "    \n",
    "    # Initialize classifiers\n",
    "    print(f\"\\n[2/4] Initializing classifiers...\")\n",
    "    classifiers = get_classifiers()\n",
    "    print(f\"      Classifiers to train: {list(classifiers.keys())}\")\n",
    "    \n",
    "    # Train and evaluate\n",
    "    print(f\"\\n[3/4] Training and evaluating models...\")\n",
    "    results = {}\n",
    "    predictions = {}\n",
    "    confusion_matrices = {}\n",
    "    \n",
    "    for name, clf in classifiers.items():\n",
    "        accuracy, y_pred, cm = evaluate_classifier(\n",
    "            name, clf, X_train_bert, X_test_bert, y_train, y_test\n",
    "        )\n",
    "        results[name] = accuracy\n",
    "        predictions[name] = y_pred\n",
    "        confusion_matrices[name] = cm\n",
    "    \n",
    "    # Baseline comparison\n",
    "    baseline_accuracy = None\n",
    "    if RUN_BASELINE and X_train_text is not None and X_test_text is not None:\n",
    "        baseline_accuracy = run_baseline_comparison(\n",
    "            X_train_text, X_test_text, y_train, y_test\n",
    "        )\n",
    "        results['Baseline (BoW)'] = baseline_accuracy\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\n[4/4] Generating summary...\")\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"FINAL RESULTS SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(f\"\\nModel Performance (Test Set Accuracy):\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    sorted_results = sorted(results.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    for rank, (name, acc) in enumerate(sorted_results, 1):\n",
    "        print(f\"{rank}. {name:30s}: {acc:.4f} ({acc*100:.2f}%)\")\n",
    "    \n",
    "    print(\"-\" * 70)\n",
    "    print(f\"\\nBest Model: {sorted_results[0][0]}\")\n",
    "    print(f\"Best Accuracy: {sorted_results[0][1]:.4f}\")\n",
    "    \n",
    "    if baseline_accuracy is not None and 'Logistic Regression' in results:\n",
    "        improvement = results['Logistic Regression'] - baseline_accuracy\n",
    "        print(f\"\\nBERT vs Baseline Improvement: {improvement:.4f} ({improvement*100:.2f}%)\")\n",
    "    \n",
    "    # Save results\n",
    "    if SAVE_RESULTS:\n",
    "        results_df = pd.DataFrame([\n",
    "            {'Model': name, 'Accuracy': acc, 'Accuracy_Percent': acc*100}\n",
    "            for name, acc in sorted_results\n",
    "        ])\n",
    "        results_df.to_csv(RESULTS_PATH, index=False)\n",
    "        print(f\"\\nResults saved to: {RESULTS_PATH}\")\n",
    "    \n",
    "    # Plot results\n",
    "    if PLOT_RESULTS:\n",
    "        plot_path = PLOT_PATH if SAVE_PLOTS else None\n",
    "        plot_results(results, save_path=plot_path)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"CLASSIFICATION COMPLETE\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    return results, predictions, confusion_matrices\n",
    "\n",
    "# ============================================================================\n",
    "# ENTRY POINT\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # If running standalone, load from files\n",
    "    if LOAD_FROM_FILES:\n",
    "        results, predictions, confusion_matrices = main()\n",
    "    else:\n",
    "        # If running after feature extraction script, pass variables\n",
    "        # Assumes X_train_bert, X_test_bert, y_train, y_test are in scope\n",
    "        try:\n",
    "            results, predictions, confusion_matrices = main(\n",
    "                X_train_bert, X_test_bert, y_train, y_test\n",
    "            )\n",
    "        except NameError:\n",
    "            print(\"\\nError: BERT features not found in memory.\")\n",
    "            print(\"Either set LOAD_FROM_FILES=True or run after feature extraction script.\")\n",
    "            print(\"Example:\")\n",
    "            print(\"  # Run feature extraction first\")\n",
    "            print(\"  exec(open('bert_feature_preparation.py').read())\")\n",
    "            print(\"  # Then run classification\")\n",
    "            print(\"  exec(open('bert_classification.py').read())\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
